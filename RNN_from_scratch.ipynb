{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.0000202660359845\n",
      "Epoch 100, Loss: 2.0000000014709256\n",
      "Epoch 200, Loss: 2.0000005293458534\n",
      "Epoch 300, Loss: 2.0000007760873237\n",
      "Epoch 400, Loss: 2.000001138365745\n",
      "Epoch 500, Loss: 2.000002160899667\n",
      "Epoch 600, Loss: 2.000005252243332\n",
      "Epoch 700, Loss: 2.000014909146844\n",
      "Epoch 800, Loss: 2.0000445300506584\n",
      "Epoch 900, Loss: 2.0001274411842243\n",
      "Predictions:\n",
      "[[[0.51420625]]\n",
      "\n",
      " [[0.51752195]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)a\n",
    "\n",
    "# Define the activation function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Initialize hyperparameters\n",
    "input_size = 2\n",
    "hidden_size = 3\n",
    "output_size = 1\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "# Initialize weights and biases\n",
    "Wxh = np.random.randn(input_size, hidden_size) * 0.01  # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01  # hidden to hidden\n",
    "Why = np.random.randn(hidden_size, output_size) * 0.01  # hidden to output\n",
    "bh = np.zeros((1, hidden_size))  # hidden bias\n",
    "by = np.zeros((1, output_size))  # output bias\n",
    "\n",
    "# Forward pass\n",
    "def forward_pass(X, h_prev):\n",
    "    hs, ys = [], []\n",
    "    h = h_prev\n",
    "    for t in range(len(X)):\n",
    "        h = sigmoid(np.dot(X[t], Wxh) + np.dot(h, Whh) + bh)  # hidden state\n",
    "        y = sigmoid(np.dot(h, Why) + by)  # output\n",
    "        hs.append(h)\n",
    "        ys.append(y)\n",
    "    return np.array(hs), np.array(ys), h\n",
    "\n",
    "# Compute loss\n",
    "def compute_loss(Y, ys):\n",
    "    return 0.5 * np.sum((Y - ys) ** 2)\n",
    "\n",
    "# Backward pass\n",
    "def backward_pass(X, Y, hs, ys, h_prev):\n",
    "    global Wxh, Whh, Why, bh, by\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dh_next = np.zeros_like(hs[0])\n",
    "\n",
    "    for t in reversed(range(len(X))):\n",
    "        dy = ys[t] - Y[t]\n",
    "        dWhy += np.dot(hs[t].reshape(-1, 1), dy)\n",
    "        dby += dy\n",
    "        dh = np.dot(dy, Why.T) + dh_next\n",
    "        dh_raw = sigmoid_derivative(hs[t]) * dh\n",
    "        dbh += dh_raw\n",
    "        dWxh += np.dot(X[t].reshape(-1, 1), dh_raw)\n",
    "        if t != 0:\n",
    "            dWhh += np.dot(hs[t-1].reshape(-1, 1), dh_raw)\n",
    "        else:\n",
    "            dWhh += np.dot(h_prev.reshape(-1, 1), dh_raw)\n",
    "        dh_next = np.dot(dh_raw, Whh.T)\n",
    "\n",
    "    # Gradient clipping to prevent exploding gradients\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -1, 1, out=dparam)\n",
    "\n",
    "    # Update weights and biases\n",
    "    Wxh -= learning_rate * dWxh\n",
    "    Whh -= learning_rate * dWhh\n",
    "    Why -= learning_rate * dWhy\n",
    "    bh -= learning_rate * dbh\n",
    "    by -= learning_rate * dby\n",
    "\n",
    "# Sample data\n",
    "X = np.array([\n",
    "    [1, 2],\n",
    "    [2, 3],\n",
    "    [3, 4],\n",
    "    [4, 5]\n",
    "])\n",
    "\n",
    "Y = np.array([\n",
    "    [0],\n",
    "    [1],\n",
    "    [0],\n",
    "    [1]\n",
    "])\n",
    "\n",
    "# Training the RNN\n",
    "h_prev = np.zeros((1, hidden_size))  # initial hidden state\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    hs, ys, h_prev = forward_pass(X, h_prev)\n",
    "    loss = compute_loss(Y, ys)\n",
    "    backward_pass(X, Y, hs, ys, h_prev)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss}')\n",
    "\n",
    "# Making predictions\n",
    "def predict(X):\n",
    "    h_prev = np.zeros((1, hidden_size))  # initial hidden state\n",
    "    _, ys, _ = forward_pass(X, h_prev)\n",
    "    return ys\n",
    "\n",
    "# Sample prediction\n",
    "X_new = np.array([\n",
    "    [5, 6],\n",
    "    [6, 7]\n",
    "])\n",
    "\n",
    "predictions = predict(X_new)\n",
    "print(\"Predictions:\")\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 636.0\n",
      "Epoch 100, Loss: 3.0147836318468806\n",
      "Epoch 200, Loss: 3.0016705204946925\n",
      "Epoch 300, Loss: 3.0006047184006155\n",
      "Epoch 400, Loss: 3.0003093698977406\n",
      "Epoch 500, Loss: 3.0001874554842365\n",
      "Epoch 600, Loss: 3.0001256242545717\n",
      "Epoch 700, Loss: 3.000090014983956\n",
      "Epoch 800, Loss: 3.000067651636078\n",
      "Epoch 900, Loss: 3.000052694646399\n",
      "Predictions:\n",
      "[[[4.64772794e-04 3.64919280e-04 3.61392048e-04 3.43196787e-04\n",
      "   3.38379246e-04 2.47938500e-04 3.21399764e-04 2.78308806e-04\n",
      "   4.41324920e-04 3.54342972e-04 3.04295433e-01 3.35455008e-04\n",
      "   3.28791588e-04 4.26942227e-04 3.51885986e-04 3.46303167e-04\n",
      "   3.18824836e-04 2.24743182e-04 3.61248742e-04 3.47248646e-04\n",
      "   2.67127483e-01 4.48779168e-04 3.86545178e-04 3.73710602e-04\n",
      "   3.46941040e-04 3.50624755e-04 2.94908848e-04 4.30997650e-04\n",
      "   3.20177204e-04 4.83900455e-04 3.34429344e-04 3.55071713e-04\n",
      "   2.96161849e-04 2.63630066e-04 2.67032459e-04 2.70776519e-04\n",
      "   3.93399087e-04 2.93507130e-04 2.31401410e-04 3.46795801e-04\n",
      "   2.53442321e-04 3.93554359e-04 3.66919107e-04 3.90961796e-04\n",
      "   3.04358446e-04 2.54740264e-04 3.33492699e-04 3.98945078e-04\n",
      "   3.82906587e-04 3.46480379e-04 4.61639684e-04 4.13315068e-04\n",
      "   2.78464309e-04 4.68656886e-04 5.44014192e-04 4.01331964e-04\n",
      "   2.54702071e-04 2.63278477e-04 4.54287965e-04 3.12262709e-04\n",
      "   3.89443781e-04 4.79129919e-04 3.66199554e-04 4.17887096e-04\n",
      "   3.41251099e-04 3.29441094e-04 4.27566656e-04 5.66205614e-04\n",
      "   4.28526647e-04 3.45183340e-04 4.21243705e-04 3.20288572e-04\n",
      "   4.45138637e-04 3.61207973e-04 5.22558255e-04 3.19967853e-04\n",
      "   3.29214256e-04 2.80416044e-04 3.42345641e-04 3.85692103e-04]]\n",
      "\n",
      " [[2.60762804e-04 2.60718854e-04 2.60780830e-04 2.59212732e-04\n",
      "   2.60207272e-04 2.59468380e-04 2.61353632e-04 2.60229822e-04\n",
      "   2.60454306e-04 2.59736490e-04 2.50000000e-01 2.59213586e-04\n",
      "   2.60060977e-04 2.59425729e-04 2.60237416e-04 2.62368890e-04\n",
      "   2.59090869e-04 2.60235585e-04 2.59231024e-04 2.59547977e-04\n",
      "   2.50000000e-01 2.60020496e-04 2.60433083e-04 2.60945578e-04\n",
      "   2.60192293e-04 2.60296542e-04 2.59276044e-04 2.59557936e-04\n",
      "   2.59025549e-04 2.57338335e-04 2.59178785e-04 2.60216053e-04\n",
      "   2.59352002e-04 2.60498034e-04 2.61002597e-04 2.60544047e-04\n",
      "   2.59702999e-04 2.60522107e-04 2.60487127e-04 2.60319528e-04\n",
      "   2.59140107e-04 2.59216889e-04 2.60542263e-04 2.60209638e-04\n",
      "   2.59930724e-04 2.60753013e-04 2.58889527e-04 2.60557263e-04\n",
      "   2.60796191e-04 2.60297181e-04 2.60515458e-04 2.60273274e-04\n",
      "   2.59377226e-04 2.60184685e-04 2.60259694e-04 2.60571200e-04\n",
      "   2.60215875e-04 2.60826716e-04 2.60548592e-04 2.59286159e-04\n",
      "   2.59858184e-04 2.58972485e-04 2.59987672e-04 2.60507161e-04\n",
      "   2.59877178e-04 2.59005785e-04 2.59742876e-04 2.60178116e-04\n",
      "   2.61227087e-04 2.59131478e-04 2.59742154e-04 2.59624762e-04\n",
      "   2.60497360e-04 2.59693991e-04 2.61102914e-04 2.59536055e-04\n",
      "   2.59439413e-04 2.61884609e-04 2.59961679e-04 2.60744422e-04]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the activation function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Initialize hyperparameters\n",
    "input_size = 2      # Number of input features\n",
    "hidden_dim = 100    # Hidden layer size\n",
    "output_dim = 80     # Number of unique words in the vocabulary\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "# Initialize weights and biases\n",
    "Wxh = np.random.uniform(0, 1, (input_size, hidden_dim))         # input to hidden\n",
    "Whh = np.random.uniform(0, 1, (hidden_dim, hidden_dim))         # hidden to hidden\n",
    "Why = np.random.uniform(0, 1, (hidden_dim, output_dim))         # hidden to output\n",
    "bh = np.zeros((1, hidden_dim))                                  # hidden bias\n",
    "by = np.zeros((1, output_dim))                                  # output bias\n",
    "\n",
    "# Forward pass\n",
    "def forward_pass(X, h_prev):\n",
    "    hs, ys = [], []\n",
    "    h = h_prev\n",
    "    for t in range(len(X)):\n",
    "        h = sigmoid(np.dot(X[t], Wxh) + np.dot(h, Whh) + bh)  # hidden state\n",
    "        y = sigmoid(np.dot(h, Why) + by)  # output\n",
    "        hs.append(h)\n",
    "        ys.append(y)\n",
    "    return np.array(hs), np.array(ys), h\n",
    "\n",
    "# Compute loss\n",
    "def compute_loss(Y, ys):\n",
    "    return 0.5 * np.sum((Y - ys) ** 2)\n",
    "\n",
    "# Backward pass\n",
    "def backward_pass(X, Y, hs, ys, h_prev):\n",
    "    global Wxh, Whh, Why, bh, by\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dh_next = np.zeros_like(hs[0])\n",
    "\n",
    "    for t in reversed(range(len(X))):\n",
    "        dy = ys[t] - Y[t]\n",
    "        dWhy += np.dot(hs[t].T, dy)\n",
    "        dby += dy\n",
    "        dh = np.dot(dy, Why.T) + dh_next\n",
    "        dh_raw = sigmoid_derivative(hs[t]) * dh\n",
    "        dbh += dh_raw\n",
    "        dWxh += np.dot(X[t].reshape(-1, 1), dh_raw.reshape(1, -1))\n",
    "        if t != 0:\n",
    "            dWhh += np.dot(hs[t-1].T, dh_raw)\n",
    "        else:\n",
    "            dWhh += np.dot(h_prev.T, dh_raw)\n",
    "        dh_next = np.dot(dh_raw, Whh.T)\n",
    "\n",
    "    # Gradient clipping to prevent exploding gradients\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -1, 1, out=dparam)\n",
    "\n",
    "    # Update weights and biases\n",
    "    Wxh -= learning_rate * dWxh\n",
    "    Whh -= learning_rate * dWhh\n",
    "    Why -= learning_rate * dWhy\n",
    "    bh -= learning_rate * dbh\n",
    "    by -= learning_rate * dby\n",
    "\n",
    "# Sample data\n",
    "X = np.array([\n",
    "    [1, 2],\n",
    "    [2, 3],\n",
    "    [3, 4],\n",
    "    [4, 5]\n",
    "])\n",
    "\n",
    "Y = np.zeros((4, output_dim))  # Sample target output with the shape (4, 80)\n",
    "\n",
    "# For demonstration, we'll set some random target outputs\n",
    "Y[1][10] = 1\n",
    "Y[3][20] = 1\n",
    "\n",
    "# Training the RNN\n",
    "h_prev = np.zeros((1, hidden_dim))  # initial hidden state\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    hs, ys, h_prev = forward_pass(X, h_prev)\n",
    "    loss = compute_loss(Y, ys)\n",
    "    backward_pass(X, Y, hs, ys, h_prev)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss}')\n",
    "\n",
    "# Making predictions\n",
    "def predict(X):\n",
    "    h_prev = np.zeros((1, hidden_dim))  # initial hidden state\n",
    "    _, ys, _ = forward_pass(X, h_prev)\n",
    "    return ys\n",
    "\n",
    "# Sample prediction\n",
    "X_new = np.array([\n",
    "    [5, 6],\n",
    "    [6, 7]\n",
    "])\n",
    "\n",
    "predictions = predict(X_new)\n",
    "print(\"Predictions:\")\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
